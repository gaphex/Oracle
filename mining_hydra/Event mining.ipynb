{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import tweepy\n",
    "import random\n",
    "import urllib2\n",
    "import Stemmer\n",
    "import sqlite3\n",
    "import operator\n",
    "import pymorphy2\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import stdout\n",
    "from lshash import LSHash\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperion():\n",
    "    def __init__(self, data):\n",
    "        print 'Invoking Hyperion...'\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.stemmer = Stemmer.Stemmer('russian')\n",
    "        self.data = data\n",
    "        \n",
    "    def processContents(self, myText):\n",
    "        myText = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', myText)\n",
    "        words = [word for word in re.findall(r'(?u)[@|#]?\\w+', myText) if not word.startswith(('@','#'))]\n",
    "        words = self.stemmer.stemWords(words)\n",
    "        return words\n",
    "        \n",
    "    def preprocess(self):\n",
    "        \n",
    "        print 'Preprocessing...this may take a while'\n",
    "        t0 = time.time()\n",
    "        terms = []\n",
    "        words = []\n",
    "\n",
    "        n = len(data.index)\n",
    "\n",
    "        for i in range(n):\n",
    "            terms.append(self.processContents(self.data.content_lower[i]))\n",
    "\n",
    "        self.data['terms'] = terms[:]\n",
    "\n",
    "        for i in range(n):\n",
    "            words += terms[i]\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        sortedDist = [x for x in sortedDist if len(x[0]) > 2]\n",
    "        interestingVocab = [x[0] for x in sortedDist]\n",
    "\n",
    "        #Find TF-IDF\n",
    "\n",
    "        trainingList = []\n",
    "        for i in range(n):\n",
    "            trainingList.append(' '.join(data['terms'][i]))\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(vocabulary = interestingVocab)\n",
    "        self.tfidf = self.vectorizer.fit_transform(trainingList)  #finds the tfidf score with normalization\n",
    "\n",
    "        print 'Building vocab and TF-IDF matrix took', time.time()-t0, 'seconds'\n",
    "        print 'Vocab Length =', len(interestingVocab), '    Vector dimensionality =', n\n",
    "        return(self.vectorizer, self.tfidf)\n",
    "\n",
    "\n",
    "    def progress(self, i, n):\n",
    "        stdout.write(\"\\r%f%%\" % (i*100/float(n)))\n",
    "        stdout.flush()\n",
    "        if i == n-1:\n",
    "            stdout.write(\"\\r100%\")\n",
    "            print(\"\\r\\n\")\n",
    "        \n",
    "    def median(self,lst):\n",
    "        return np.median(np.array(lst))\n",
    "\n",
    "    def extract_urls(self, lst):\n",
    "        urls = []\n",
    "        for i in lst:\n",
    "            for j in i.split(' '):\n",
    "                if j.startswith('http'):\n",
    "                    urls.append(j)\n",
    "        return urls\n",
    "\n",
    "    def resolve_url(self, starturl):\n",
    "        try:\n",
    "            req = urllib2.Request(starturl)\n",
    "            res = urllib2.urlopen(req, timeout = 2)\n",
    "            finalurl = res.geturl()\n",
    "            return finalurl\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def setup(self, vectorizer, tf_idf):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tfidf = tf_idf\n",
    "        print 'Setup complete '\n",
    "        \n",
    "\n",
    "    def findSimilarTweets(self, queryTweet, threshold, maxNumber = 0, log = False):\n",
    "\n",
    "        t0 = time.time()\n",
    "        \n",
    "        processedTweet = ' '.join(self.processContents(queryTweet))\n",
    "        queryTweetRepresentation = self.vectorizer.transform([processedTweet])\n",
    "\n",
    "        cosine_similarities = cosine_similarity(queryTweetRepresentation, self.tfidf)[0]\n",
    "        totalMatchingTweets = len(cosine_similarities[cosine_similarities>threshold])\n",
    "\n",
    "        if maxNumber:\n",
    "            totalMatchingTweets = min(totalMatchingTweets, maxNumber)\n",
    "        indices = cosine_similarities.argsort()[::-1][:totalMatchingTweets]\n",
    "        elapsed_time = time.time() - t0\n",
    "        if len(indices) > 25:\n",
    "            print 'Query:', queryTweet\n",
    "        return indices\n",
    "\n",
    "    def findOut(self, cluster):\n",
    "        lst = list(self.data['content'][cluster])\n",
    "\n",
    "        redirects = []\n",
    "        keywords = []\n",
    "\n",
    "        urls = self.extract_urls(lst)\n",
    "        for url in urls:\n",
    "            redirects.append(self.resolve_url(url))\n",
    "\n",
    "        corpus = ' '.join(lst).lower()\n",
    "        corpus = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', corpus)\n",
    "        words = []\n",
    "        tokenized = nltk.tokenize.word_tokenize(corpus)\n",
    "        for word in tokenized:\n",
    "            words.append(word)\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        moreThan3 = [x for x in sortedDist if len(x[0]) > 3 and x[1] > 3]\n",
    "        for i in moreThan3:\n",
    "            keywords.append(i[0])\n",
    "\n",
    "        return(keywords, redirects)\n",
    "\n",
    "    def resolve(self, masterCluster):\n",
    "        t0 = time.time()\n",
    "        k, r = self.findOut(masterCluster)\n",
    "\n",
    "        insta = []\n",
    "        swarm = []\n",
    "\n",
    "        print 'Found', len(r), 'anonymous links, investigating...'\n",
    "        for i, u in enumerate(r):\n",
    "            if u:\n",
    "                if 'instagram' in u:\n",
    "                    insta.append(u)\n",
    "                if 'swarmapp' in u:\n",
    "                    swarm.append(u)\n",
    "\n",
    "        print 'Resolving took', time.time()-t0\n",
    "        return ((k, insta, swarm))\n",
    "\n",
    "    def overlap(self, r1l, r1r, r1t, r1b, r2l, r2r, r2t, r2b):\n",
    "        #Overlapping rectangles overlap both horizontally & vertically\n",
    "        return self.range_overlap(r1l, r1r, r2l, r2r) and self.range_overlap(r1b, r1t, r2b, r2t)\n",
    "\n",
    "    def range_overlap(self, a_min, a_max, b_min, b_max):\n",
    "        #Neither range is completely greater than the other\n",
    "        return (a_min <= b_max) and (b_min <= a_max)\n",
    "\n",
    "    def assess(self, clusters):\n",
    "        data = []\n",
    "        for c in clusters:\n",
    "\n",
    "            xs = [t[0] for t in c]\n",
    "            ys = [t[1] for t in c]\n",
    "            z = zip(xs, ys)\n",
    "            n = len(z)\n",
    "\n",
    "            dx = max(xs) - min(xs)\n",
    "            dy = max(ys) - min(ys)\n",
    "\n",
    "            mx = self.median(xs)\n",
    "            my = self.median(ys)\n",
    "\n",
    "            s=(dx+0.001)*(dy+0.001)\n",
    "            score = n/s\n",
    "\n",
    "            data.append((z,n,score,dx,dy,mx,my))\n",
    "            data = filter(lambda a: a[1] > 1, data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def findUnique(self, stats):\n",
    "\n",
    "        for v in range(3):\n",
    "            for i in stats:\n",
    "                r1l = i[5]-1.2*i[3]\n",
    "                r1r = i[5]+1.2*i[3]\n",
    "                r1b = i[6]-1.2*i[4]\n",
    "                r1t = i[6]+1.2*i[4]\n",
    "                for j in stats:\n",
    "                    r2l = j[5]-1.2*j[3]\n",
    "                    r2r = j[5]+1.2*j[3]\n",
    "                    r2b = j[6]-1.2*j[4]\n",
    "                    r2t = j[6]+1.2*j[4]\n",
    "                    if self.overlap(r1l, r1r, r1t, r1b, r2l, r2r, r2t, r2b) and i!=j:\n",
    "                        if i[1] >= j[1]:\n",
    "                            try:\n",
    "                                stats.remove(j)\n",
    "                            except:\n",
    "                                pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                stats.remove(i)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "        return stats\n",
    "    \n",
    "    def performClusterisation(self, indices, thSp, thTm):\n",
    "    \n",
    "        #Spatial Clustering\n",
    "        y = list(data['lat'][indices])\n",
    "        x = list(data['long'][indices])\n",
    "        tid = indices\n",
    "\n",
    "        z = filter(lambda a: a[0] != (55.753301, 37.619899), zip(zip(x,y), tid))\n",
    "        x = []\n",
    "        y = []\n",
    "        tid = []\n",
    "\n",
    "        for t in z:\n",
    "            x.append(t[0][0])\n",
    "            y.append(t[0][1])\n",
    "            tid.append(t[1])\n",
    "\n",
    "        crds = zip(x,y)\n",
    "        spUniques = self.findCluster(crds, tid, 34, thSp, thTm, 'space')\n",
    "        if len(spUniques):\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', len(spUniques), 'spatial clusters with a total of', sum([len(t[0]) for t in spUniques]), 'points among', len(indices)\n",
    "\n",
    "        for w in spUniques:\n",
    "            print len(w[0]), 'points at', w[3], w[4], buildURI(w[3], w[4])\n",
    "\n",
    "\n",
    "        #Temporal Clustering\n",
    "        tmvc = []\n",
    "        for t in data['created_at'][indices]:\n",
    "            l = t.split(' ')\n",
    "            date = l[0].split('-')\n",
    "            time = l[1].split(':')\n",
    "            datehash = int(date[1]) * 30 + int(date[2])\n",
    "            timehash = int(time[0]) * 3600 + int(time[1]) * 60 + int(time[2])\n",
    "            tmvc.append((datehash*400, timehash))\n",
    "\n",
    "        tpUniques = self.findCluster(tmvc, tid, 34, thSp, thTm, 'time')\n",
    "        dm = len(tpUniques)\n",
    "\n",
    "        if dm:\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', dm, 'temporal clusters with a total of', sum([len(t[0]) for t in tpUniques]), 'points among', len(indices)\n",
    "        \n",
    "\n",
    "        for w in tpUniques:\n",
    "            date = w[3]/400\n",
    "            time = w[4]\n",
    "            print len(w[0]), 'points at', int(date//30),'.',int(date%30),'--',int(time//3600),\n",
    "            print ':',int(time%3600//60),':',int(time%3600%60)\n",
    "\n",
    "        print ''\n",
    "        return ((spUniques, tpUniques))\n",
    "\n",
    "    def findCluster(self, vecs, tids, cln, thSp, thTm, mode):\n",
    "\n",
    "        maxn = 0\n",
    "        maxc = []\n",
    "        dim = len(vecs[0])\n",
    "        data = zip(vecs, tids)\n",
    "\n",
    "\n",
    "        if mode == 'space':\n",
    "            metric = \"euclidean\"\n",
    "            th = thSp\n",
    "            rat = 1\n",
    "        if mode == 'time':\n",
    "            metric = 'hamming'\n",
    "            th = thTm\n",
    "            rat = 1\n",
    "\n",
    "        lsh = LSHash(cln, dim)\n",
    "        for i in vecs:\n",
    "            lsh.index(i)\n",
    "\n",
    "        pivots = list(set(random.sample(vecs, int(rat*len(vecs)))))\n",
    "\n",
    "        for i in pivots:\n",
    "            mxc = []\n",
    "            dists = []\n",
    "            u = lsh.query(i, distance_func=metric)\n",
    "\n",
    "            for j in u:\n",
    "                dists.append(j[1])\n",
    "                mxc.append(j[0])\n",
    "\n",
    "            r = self.cover(dists, th)\n",
    "            maxc.append(mxc[0:r])\n",
    "\n",
    "\n",
    "        scr = self.assess(maxc)\n",
    "        rez = self.findUnique(scr)\n",
    "\n",
    "        clusters = []\n",
    "        for u in rez:\n",
    "            cIDs = []\n",
    "            for t in u[0]:\n",
    "                for d in data:\n",
    "                    if t == d[0]:\n",
    "                        cIDs.append(d[1])\n",
    "            clusters.append((cIDs, u[3], u[4], u[5], u[6]))\n",
    "\n",
    "        clusters = filter(lambda a: len(a[0]) > 1, clusters)\n",
    "\n",
    "        return (clusters)\n",
    "\n",
    "    def cover(self, dat, th):\n",
    "        sum = 0\n",
    "        t = 0\n",
    "        while sum < th*len(dat) and t < 0.5*len(dat):\n",
    "            sum += dat[t]\n",
    "            t += 1\n",
    "        return t\n",
    "\n",
    "    def doQuery(self, query, th_NN, th_SP, th_TM):\n",
    "        t0 = time.time()\n",
    "        indices = self.findSimilarTweets(query, th_NN)\n",
    "        print len(indices), 'points passed preprocessing'\n",
    "\n",
    "        if len(indices):\n",
    "            t = self.performClusterisation(indices, th_SP, th_TM)\n",
    "            print \"Query processing took\", time.time()-t0, 'seconds'\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n"
     ]
    }
   ],
   "source": [
    "hyperion = Hyperion(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...this may take a while\n",
      "Building vocab and TF-IDF matrix took 165.14781189 seconds\n",
      "Vocab Length = 414550     Vector dimensionality = 1315775\n"
     ]
    }
   ],
   "source": [
    "vec, tf = hyperion.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: я в кинотеатре кино премьера\n",
      "317 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 12 spatial clusters with a total of 87 points among 317\n",
      "5 points at 55.834837 37.571329 https://www.google.ru/maps/place/55.834837,37.571329\n",
      "4 points at 55.856801 37.6528515 https://www.google.ru/maps/place/55.856801,37.6528515\n",
      "8 points at 55.75766 37.61768 https://www.google.ru/maps/place/55.75766,37.61768\n",
      "3 points at 55.876694 37.693955 https://www.google.ru/maps/place/55.876694,37.693955\n",
      "8 points at 55.81912 37.637436 https://www.google.ru/maps/place/55.81912,37.637436\n",
      "3 points at 55.744115 37.565458 https://www.google.ru/maps/place/55.744115,37.565458\n",
      "6 points at 55.914017 37.803819 https://www.google.ru/maps/place/55.914017,37.803819\n",
      "6 points at 55.709025 37.8878415 https://www.google.ru/maps/place/55.709025,37.8878415\n",
      "7 points at 55.759723 37.657296 https://www.google.ru/maps/place/55.759723,37.657296\n",
      "7 points at 55.772006 37.588241 https://www.google.ru/maps/place/55.772006,37.588241\n",
      "3 points at 55.699792 37.764953 https://www.google.ru/maps/place/55.699792,37.764953\n",
      "27 points at 55.752867 37.587791 https://www.google.ru/maps/place/55.752867,37.587791\n",
      "--------------------------------------------------------------\n",
      "Found 16 temporal clusters with a total of 55 points among 317\n",
      "6 points at 4 . 15 -- 16 : 26 : 58\n",
      "3 points at 5 . 18 -- 16 : 44 : 44\n",
      "3 points at 5 . 18 -- 11 : 17 : 34\n",
      "3 points at 4 . 15 -- 20 : 5 : 16\n",
      "3 points at 4 . 22 -- 16 : 21 : 58\n",
      "4 points at 4 . 27 -- 20 : 0 : 52\n",
      "4 points at 4 . 25 -- 14 : 35 : 44\n",
      "3 points at 4 . 15 -- 14 : 59 : 10\n",
      "3 points at 4 . 18 -- 22 : 31 : 3\n",
      "3 points at 5 . 20 -- 18 : 0 : 51\n",
      "4 points at 3 . 26 -- 17 : 18 : 3\n",
      "3 points at 5 . 22 -- 23 : 2 : 6\n",
      "3 points at 5 . 19 -- 15 : 40 : 3\n",
      "4 points at 4 . 26 -- 11 : 34 : 3\n",
      "3 points at 4 . 11 -- 8 : 57 : 50\n",
      "3 points at 5 . 8 -- 13 : 3 : 0\n",
      "\n",
      "Query processing took 2.51686406136 seconds\n"
     ]
    }
   ],
   "source": [
    "u = hyperion.doQuery(u'я в кинотеатре кино премьера', 0.34, 0.00000005, 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(self.data['content'][cluster])\n",
    "\n",
    "        redirects = []\n",
    "        keywords = []\n",
    "\n",
    "        urls = self.extract_urls(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Setup complete \n"
     ]
    }
   ],
   "source": [
    "hyperion = Hyperion(data)\n",
    "hyperion.setup(vec, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'https://t.co/S47kczdGup', u'https://t.co/uPpBP0kr74', u'https://t.co/15h6BcMULm', u'https://t.co/J5g6pga1KI', u'https://t.co/Nbd6DK0wp9', u'http://t.co/qCDpoN3F7i', u'https://t.co/QwlzdYWVMG', u'http://t.co/V0rsmfGB0b', u'https://t.co/i0Co0yMYAu', u'http://t.co/pG7ZKxW78Y', u'http://t.co/PzfJHDEK9e', u'https://t.co/zY6YTUuuyI', u'https://t.co/Aey7qR0sDz', u'https://t.co/LYZxsfoCy9', u'https://t.co/HY7wE2kqTY', u'https://t.co/son7wcXPPL', u'https://t.co/HXNjVNKP05', u'https://t.co/wWfSXhgrft', u'https://t.co/RvfNzj6bi9', u'https://t.co/MpAWvS6HM5', u'https://t.co/TmYGsqzK7k', u'https://t.co/i3FvZVEVll', u'https://t.co/p7PvE4ZhJG', u'https://t.co/ujTKPXTn9H', u'https://t.co/3w4fdwRt3y', u'https://t.co/ha1l2HnzMK', u'https://t.co/g05A2gGdtA', u'http://t.co/mq5jGOw6R5', u'http://t.co/i0vgaLJ5hD', u'http://t.co/agETaUUbEW', u'https://t.co/JheqN9Sf0M', u'https://t.co/kDf1u5YtGE', u'https://t.co/rVtvhRhPNH']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "media = []\n",
    "t = 0\n",
    "for clusterType in u:\n",
    "    for cluster in clusterType:\n",
    "        lst = list(hyperion.data['content'][cluster[0]])\n",
    "        urls = hyperion.extract_urls(lst)\n",
    "        \n",
    "        t += len(urls)\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def buildURI(x, y):\n",
    "    return \"https://www.google.ru/maps/place/\" + str(x) + ',' + str(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "CONSUMER_KEY = ['KpfGPpsl5Dn03Lb5wzvQfEaMc',\n",
    "                '13AqFSrFdFv7rdLVOGvzJCkmp',\n",
    "                '45RuEYLg5eVTYyEGuyEerplyY',\n",
    "                '3qTMdAYKxctRe69HMDqyeNST2',\n",
    "                'JJkrFkKGlhDIEgj2eTrQ']\n",
    "\n",
    "CONSUMER_SECRET = ['UWRvjR3CHsducO1i7268F24C3M9UJu5U7p2u4kh6Ds6QMDdKCg',\n",
    "                   'LVaOSMsMBWl4FmgthjNPWMnkKe7MXKXrmu5uL6JnJWIhHieDxR',\n",
    "                   'LBnbBTIAhtYYBU6RWeyCzgIcJannob7bPrzg3dMqFuRDLJnbHp',\n",
    "                   'Jwwv3wHzL2jtYMHylakpjmDxf5SgvAwexFGfEoCFHw92f65lnK',\n",
    "                   'H7hmUQXqXseKbj1WnKFMnaURyQbBaDeyK3DAAwLI']\n",
    "\n",
    "OAUTH_TOKEN = ['2181757628-o8IOmHBelyhVM6KEkkT50ZLIbv4fj6llW6KSjpd',\n",
    "               '175663996-ZNL1MivJASYSxWsNXlxNHnQhmLHDegH9VdVfATsL',\n",
    "               '175663996-lQRf1JNjvR1fVILTtoEH4FHVQ1sLtPa0IIa8lMog',\n",
    "               '2841198550-rlPUcMyCj8rk3Yv6XxGJWk0ELCCUGUrxhvYyAa6',\n",
    "               '2181757628-0n3FpGEtoob0qum7IMeN3R0oV1kg5STZwmXNa9Q']\n",
    "\n",
    "OAUTH_TOKEN_SECRET = ['cyLlZtQyv4rgcWA5pGaXLtGJaFqD4PGOlxSdb4ECVzoSP',\n",
    "                      'gJOHvvlcObkiu7Qd91WapTFwnOVsisdoeMBUHxcFfzBac',\n",
    "                      'eQ0SUwziSUgRs72HJzWpU9IAlVP92X9YJsGHOPrWUctw3',\n",
    "                      '9b36g1wXLzn1yB0FGIoT9eACxPPpaZVfESnmRYcDYk3wv',\n",
    "                      'MqgrZHb8CMyNqFJn36YmCtLUQ5rqNUzX2IxWNfQdHQ6t7']\n",
    "\n",
    "auths = []\n",
    "\n",
    "up = 55.96\n",
    "down = 55.49\n",
    "right = 37.97\n",
    "left = 37.32\n",
    "\n",
    "for i in range(5):\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY[i], CONSUMER_SECRET[i])\n",
    "    auth.set_access_token(OAUTH_TOKEN[i], OAUTH_TOKEN_SECRET[i])\n",
    "    auths.append(auth)\n",
    "\n",
    "\n",
    "\n",
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            tid = status.id_str\n",
    "            usr = status.author.screen_name.strip()\n",
    "            txt = status.text.strip()\n",
    "\n",
    "            indices = hyperion.findSimilarTweets(txt, 0.5)\n",
    "            \n",
    "            if len(indices) > 34:\n",
    "                try:\n",
    "                    hyperion.doQuery(txt, 0.34, 0.000005, 5000)\n",
    "                except:\n",
    "                    print 'Caught exception during clusterization'\n",
    "    \n",
    " \n",
    "        except Exception as e:\n",
    "            # Most errors we're going to see relate to the handling of UTF-8 messages (sorry)\n",
    "            print('BULLSHIT', e)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_disconnect(self, notice):\n",
    "        print notice\n",
    "        return True # Don't kill the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapi = tweepy.streaming.Stream(auths[2], CustomStreamListener())\n",
    "sapi.filter(locations=[left, down, right, up])fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ['123123', '123', '23443']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "sum([len(t) for t in u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}