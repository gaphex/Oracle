{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\")\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import _sqlite3\n",
    "import operator\n",
    "import itertools\n",
    "import numpy as np\n",
    "import collections\n",
    "import fastcluster\n",
    "import CMUTweetTagger\n",
    "from emoji import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from stopwords import stopwords\n",
    "from IPython.display import clear_output\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def load_stop_words():\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words.extend(stopwords)\n",
    "    return set(stop_words)\n",
    "\n",
    "def extract_hashtags(dset):\n",
    "    htags = []\n",
    "    for d in dset:\n",
    "        ht = d['payload']['entities']['hashtags']\n",
    "        if len(ht):\n",
    "            for i in ht:\n",
    "                htags.append(i['text'])\n",
    "    return htags\n",
    "\n",
    "def tovoc(sent):\n",
    "    vocvec = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            i = cp.w2v_dictionary[word]\n",
    "            vocvec.append(i)\n",
    "        except:\n",
    "            pass\n",
    "    return vocvec\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1/(1+exp(-t))\n",
    "\n",
    "def list_intersection(l1, l2):\n",
    "    return set(l1)&set(l2)\n",
    "\n",
    "def sent2vec(sent):\n",
    "    M = [cp.w2v_embeddings[v] for v in tovoc(sent)]\n",
    "    try:\n",
    "        r = np.sum(M, axis = 0)\n",
    "        return r\n",
    "    except:\n",
    "        print 'OOV'\n",
    "        return 0\n",
    "        \n",
    "def cossim(s1, s2):\n",
    "    v1 = sent2vec(s1)\n",
    "    v2 = sent2vec(s2)\n",
    "    if not v1.shape or not v2.shape:\n",
    "        return [[0]]\n",
    "    return cosine_similarity(v1, v2)\n",
    "\n",
    "def progress(i, n, skip = 100, mode = 1):\n",
    "    if i%skip == 0 and mode == 1:\n",
    "        sys.stdout.write(\"\\r%s%%\" % \"{:5.2f}\".format(100*i/float(n)))\n",
    "        sys.stdout.flush()\n",
    "        if i >= (n/skip - 1)*skip:\n",
    "            sys.stdout.write(\"\\r100.00%\")\n",
    "            print(\"\\r\")\n",
    "    if i%skip == 0 and mode == 2:\n",
    "        sys.stdout.write(\"\\r%s\" % str(i))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "\n",
    "def db_count(con):\n",
    "    curs = con.cursor()\n",
    "    table = 'tweets'\n",
    "    rowsQuery = \"SELECT Count() FROM %s\" % table\n",
    "    curs.execute(rowsQuery)\n",
    "    numberOfRows = curs.fetchone()[0]\n",
    "    return numberOfRows\n",
    "\n",
    "def locate(obj):\n",
    "    if obj['coordinates']:\n",
    "        return 1, obj['coordinates']['coordinates']\n",
    "    else:\n",
    "        box = obj['place']['bounding_box']['coordinates']\n",
    "        m1, m2 = list((np.sum(box, axis = 1)/4)[0])\n",
    "        s1, s2 = list((np.array(box[0])[2] - np.array(box[0])[0])/2)\n",
    "        #return 0, [random.gauss(m1, s1), random.gauss(m2,s2)]\n",
    "        return 0, [m1, m2]\n",
    "\n",
    "def resolveEmoji(myText):\n",
    "    emostr = []\n",
    "    emo_db = emoji\n",
    "    b = myText.encode('unicode_escape').split('\\\\')\n",
    "    c = [point.replace('000','+').upper() for point in b if len(point) > 8 and point[0] == 'U']\n",
    "    emj = [(emo_db[emo[:7]]) for emo in c if emo[:7] in emo_db]\n",
    "    return emj\n",
    "\n",
    "def extract_links(myText, tokens=False):\n",
    "    links = []\n",
    "    rex = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n",
    "    for rf in re.findall(rex, myText):\n",
    "        for t in rf:\n",
    "            if len(t)>2:\n",
    "                links.append(t)\n",
    "    if tokens:\n",
    "        c = '+LINK'\n",
    "    else:\n",
    "        c = ' '\n",
    "    return ((re.sub(rex, c, myText), links))\n",
    "\n",
    "def w2v_transform(ls):\n",
    "    M = []\n",
    "    for i in ls:\n",
    "        try:\n",
    "            t = sent2vec(i)\n",
    "            if t.shape[0] == 512:\n",
    "                M.append(sent2vec(i))\n",
    "        except:\n",
    "            pass\n",
    "    M = np.array(M)\n",
    "    return M\n",
    "\n",
    "def get_last(collection, hours=0, count =0):\n",
    "    tshift = 5\n",
    "    cur = ms.db[collection].find().sort(\"_id\", -1)\n",
    "    if hours:\n",
    "        time_now = time.mktime(time.localtime())\n",
    "        for i, doc in enumerate(cur):\n",
    "            if i%1000 == 0:\n",
    "                if i==-1:\n",
    "                    time_now = time.mktime(time.strptime(doc['payload']['created_at'], '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "                created_at = time.mktime(time.strptime(doc['payload']['created_at'], '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "                dif = tshift*3600 + time_now - created_at\n",
    "                #print dif - hours*3600\n",
    "                if dif > hours*3600:\n",
    "                    break\n",
    "        print 'retrieved', i, collection, 'tweets', 'posted in', hours, 'hours'\n",
    "        return list(ms.db[collection].find().sort(\"_id\", -1).limit(i))\n",
    "    if count:\n",
    "        print 'retrieved', count, 'tweets in', collection\n",
    "        return list(ms.db[collection].find().sort(\"_id\", -1).limit(count))\n",
    "    \n",
    "def filter_noisy(ms, dev, w=3, m=3, h=4):\n",
    "    cprc = {}\n",
    "    cdev = []\n",
    "    stophash = ['job', 'hiring', 'Job', 'Hiring']\n",
    "    for i, d in  enumerate(dev):\n",
    "        prc = ms.process(d)\n",
    "        if len(prc['words'])>w and len(prc['mentions'])<m and len(prc['hashtags'])<h and prc['lang'] == u'en':\n",
    "            if not len(list_intersection(prc['hashtags'], stophash)):\n",
    "                cprc[len(cdev)] = prc\n",
    "                cdev.append(d)\n",
    "    print 'filtered', len(dev)-len(cdev), 'documents,', len(cdev), 'left'\n",
    "    return cdev, cprc\n",
    "\n",
    "def filter_rare(X, th=3):\n",
    "    mapping = {}\n",
    "    Xclean = np.zeros((1, X.shape[1]))\n",
    "    for i in range(X.shape[0]):\n",
    "        if X[i].sum() > th:\n",
    "            Xclean = np.vstack([Xclean, X[i].toarray()])\n",
    "            mapping[Xclean.shape[0] - 2] = i\n",
    "        progress(i, X.shape[0])\n",
    "\n",
    "    Xclean = Xclean[1:,]\n",
    "    return Xclean, mapping\n",
    "\n",
    "def filter_coords(cdev, cdict):\n",
    "    cr = []\n",
    "    cd = {}\n",
    "    mapping = {}\n",
    "    for i, c in enumerate(cdev):\n",
    "        if c['payload']['coordinates']:\n",
    "            cd[len(cr)] = cdict[i]\n",
    "            mapping[len(cr)] = i\n",
    "            cr.append(c)\n",
    "    print 'filtered', len(cdev)-len(cr), 'documents,', len(cr), 'left'\n",
    "    return cr, cd, mapping\n",
    "\n",
    "def organize_clusters(fcl, top_n = 500, th = 4):\n",
    "    dc = {}\n",
    "    top_clusters = []\n",
    "    for i in range(max(fcl)+1):\n",
    "        dc[i] = []\n",
    "    for i, f in enumerate(fcl):\n",
    "        dc[f].append(i)\n",
    "\n",
    "    dd = [(d[0], len(dc[d[1]])) for d in enumerate(dc)]\n",
    "    d1 = sorted(dd, key=itemgetter(1), reverse = True)\n",
    "    for d in d1[0:top_n]:\n",
    "        if d[1] > th:\n",
    "            top_clusters.append(dc[d[0]])\n",
    "        else:\n",
    "            break\n",
    "    return top_clusters\n",
    "\n",
    "def process_entities(cdev, cprc):\n",
    "    ent_corpus = []\n",
    "    hdev = []\n",
    "    hprc = {}\n",
    "    mapping = {}\n",
    "    er = []\n",
    "\n",
    "    for i in cprc:\n",
    "        entities = cprc[i]['hashtags'] + cprc[i]['checkins']\n",
    "        if entities:\n",
    "            ent_corpus += entities\n",
    "            if list_intersection(entities, cprc[i]['words']):\n",
    "                hprc[len(hdev)] = cprc[i]\n",
    "                mapping[len(hdev)] = i\n",
    "                hdev.append(' '.join(entities))\n",
    "            else:\n",
    "                er.append(i)\n",
    "    return hdev, hprc, mapping, ent_corpus, er\n",
    "\n",
    "def boost_entities(features):\n",
    "    boost_entity = {}\n",
    "    pos_tokens = CMUTweetTagger.runtagger_parse([term.upper() for term in features])\n",
    "\n",
    "    for line in pos_tokens:\n",
    "        term =''\n",
    "        for entity in range(len(line)):\n",
    "            term += line[entity][0].lower() + \" \"\n",
    "            if \"^\" in str(line):\n",
    "                boost_entity[term.strip()] = 2.5\n",
    "            else:\n",
    "                boost_entity[term.strip()] = 1.0\n",
    "    return boost_entity\n",
    "\n",
    "def build_voc(corp, thr):\n",
    "    cnt = collections.Counter(corp).most_common()\n",
    "    voc = []\n",
    "    for i, c in enumerate(cnt):\n",
    "        if c[1]<thr:\n",
    "            break\n",
    "    for c in cnt[0:i]:\n",
    "        voc.append(c[0])\n",
    "    return voc\n",
    "\n",
    "def connect_to_mongo(port, host = 'localhost'):\n",
    "        client = MongoClient(host, port)\n",
    "        if str(client).split('=')[-1][:-1] == 'True':\n",
    "            print 'connected'\n",
    "            return client\n",
    "        else:\n",
    "            print 'not connected'\n",
    "            return 0\n",
    "        \n",
    "def retrieve_from_timewindow(ms, col, startEpoch, endEpoch):\n",
    "    print 'querying..'\n",
    "    m = ms.db[col].find({\"payload.created_at\":{'$gte':startEpoch, '$lte':endEpoch}})\n",
    "    dev = []\n",
    "    print 'retrieving..'\n",
    "    for i, obj in enumerate(m):\n",
    "        progress(i, 1, mode = 2)\n",
    "        dev.append(obj)\n",
    "    print ''\n",
    "    print 'retrieved', len(dev), 'documents from window'\n",
    "    return dev\n",
    "        \n",
    "\n",
    "class WD_PROC_7000(object):\n",
    "    def __init__(self, db, mode, port = None):\n",
    "        \n",
    "        self.client = connect_to_mongo(port)\n",
    "        self.db = self.client[db]\n",
    "        self.s_words = load_stop_words()\n",
    "        self.mode = mode\n",
    "        self.bag = []\n",
    "        \n",
    "    def db_count(self):\n",
    "        return [{col: self.db[col].count()} for col in self.db.collection_names()]\n",
    "    \n",
    "    def sum_db_count(self):\n",
    "        return sum([t.values()[0] for t in self.db_count()])\n",
    "        \n",
    "    def __iter__(self):\n",
    "\n",
    "        g = self.db.collection_names()\n",
    "        for col in g[0:-1]:\n",
    "            for obj in self.db[col].find({}):\n",
    "                #yield self.process(obj)\n",
    "                yield(obj)\n",
    "                \n",
    "    def tokenise(self, myText, tokens=False, usestop = True):\n",
    "        words, htags, ments = [], [], []\n",
    "        for word in re.findall(r'(?u)[@|#]?\\w+', myText):\n",
    "            if word in ['+LINK', '+HASH', '+MENT']:\n",
    "                pass\n",
    "            else:\n",
    "                word = word.lower()\n",
    "            if usestop:\n",
    "                if word not in self.s_words:    \n",
    "                    words.append(word)\n",
    "            else:\n",
    "                words.append(word)\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if word.startswith('#'):\n",
    "                words[i] = word.split('#')[1]\n",
    "                htags.append(word)\n",
    "                if tokens:\n",
    "                    words.insert(i, '+HASH')\n",
    "\n",
    "            if word.startswith('@'):\n",
    "                ments.append(word)\n",
    "                words[i] = ''\n",
    "                if tokens:\n",
    "                    words[i] = '+MENT' \n",
    "\n",
    "        return [word for word in words if len(word)]\n",
    "\n",
    "        \n",
    "    def process(self, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            try:\n",
    "                id = obj['_id']\n",
    "                payload = obj['payload']\n",
    "                raw_text = payload['text']\n",
    "\n",
    "                q, crds = locate(payload)\n",
    "                tstmp = time.strptime(payload['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                \n",
    "                ent = payload['entities']\n",
    "                urls, media_urls, hashtags, mentions = [],[],[],[]\n",
    "                if ent:\n",
    "                    for et in ent.keys():\n",
    "                        \n",
    "                        if len(ent[et]) and et == 'urls':\n",
    "                            urls = [i['expanded_url'] for i in ent['urls']]\n",
    "                        if len(ent[et]) and et == 'hashtags':\n",
    "                            hashtags = [i['text'] for i in ent['hashtags']]\n",
    "                        if len(ent[et]) and et == 'user_mentions':\n",
    "                            mentions = [i['screen_name'] for i in ent['user_mentions']]\n",
    "                        if len(ent[et]) and et == 'media':\n",
    "                            media_urls = [i['expanded_url'] for i in ent['media']]\n",
    " \n",
    "\n",
    "                text, links = extract_links(raw_text)\n",
    "                words = self.tokenise(text)\n",
    "            \n",
    "                checkins = []\n",
    "                bag = text.split(' ')\n",
    "                if '@' in bag:\n",
    "                    ind = bag.index('@')\n",
    "                    for t in range(ind, len(bag)):\n",
    "                        if len(bag[t]):\n",
    "                            if bag[t][0].isupper():\n",
    "                                plo = remove_punctuation(bag[t].encode('ascii','ignore').lower())\n",
    "                                plo = unicode(plo)\n",
    "                                checkins.append(plo)\n",
    "\n",
    "                #emoji = resolveEmoji(text)\n",
    "                lang = payload['lang']\n",
    "                tstamp = time.mktime(tstmp)\n",
    "                        \n",
    "                return {'id':int(id), 'words':words, 'hashtags':hashtags, 'checkins':checkins, 'mentions':mentions, 'ctype':q, 'crds':crds, 'timestamp':tstamp, 'lang':lang, 'urls':urls+media_urls}\n",
    "\n",
    "            except Exception as e:\n",
    "                print 'could not unpack object', obj['_id'], e\n",
    "                return None\n",
    "            \n",
    "    def add_to_bag(self, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            try:\n",
    "                payload = obj['payload']\n",
    "                raw_text = payload['text']\n",
    "                \n",
    "                text, links = extract_links(raw_text)\n",
    "                words, _, _ = self.tokenise(text)\n",
    "\n",
    "                self.bag += words\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering models and cluster processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DB_SCAN(GM, th):\n",
    "\n",
    "    db = DBSCAN(eps=th, min_samples=3).fit(GM)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    gd = {}\n",
    "    \n",
    "    for i in range(0, max(labels)+1):\n",
    "        gd[i] = []\n",
    "\n",
    "    _ = [gd[label].append(i) for i, label in enumerate(labels) if label!=-1]\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    return gd\n",
    "\n",
    "def run_geoclust_model(cdev, cprc, th = 0.03):\n",
    "    print '____________________________________________________'\n",
    "    print 'running geo model'\n",
    "    gdev, gprc, gmapping = filter_coords(cdev, cprc)\n",
    "    GM = []\n",
    "    for i in gprc:\n",
    "        GM.append(gprc[i]['crds'])\n",
    "    GM = np.array(GM)\n",
    "    X = StandardScaler().fit_transform(GM)\n",
    "\n",
    "    gc = DB_SCAN(X, th)\n",
    "    gcf = []\n",
    "    for g in gc:\n",
    "        gcf.append([gmapping[t] for t in gc[g]])\n",
    "        \n",
    "    print 'detected', len(gcf), 'geo clusters' \n",
    "    return gcf\n",
    "\n",
    "\n",
    "def run_entity_model(cdev, cprc):\n",
    "    print '____________________________________________________'\n",
    "    print 'running entity model'\n",
    "    hdev, hprc, hmapping, entcorp, er = process_entities(cdev, cprc)\n",
    "    print 'removed', len(cdev)- len(hdev), 'documents', len(hdev), 'left'\n",
    "    voc = build_voc(entcorp, 2)\n",
    "    \n",
    "    ent_vectorizer = CountVectorizer(vocabulary = voc)\n",
    "    E = ent_vectorizer.fit_transform(hdev)\n",
    "    \n",
    "    Eclean, emapping = filter_rare(E, 0)\n",
    "\n",
    "    E_dense = np.matrix(Eclean).astype('float')\n",
    "    E_scaled = preprocessing.scale(E_dense)\n",
    "    E_normalized = preprocessing.normalize(E_scaled, norm='l2')\n",
    "    \n",
    "    EMatrix = pairwise_distances(E_normalized, metric='cosine')\n",
    "    EL = fastcluster.linkage(EMatrix, method='average')\n",
    "    flat_eclust = hierarchy.fcluster(EL, 0.5, 'distance')\n",
    "    ec = organize_clusters(flat_eclust, th = 3)\n",
    "    \n",
    "    ecf = []\n",
    "    for cl in ec:\n",
    "        ecf.append([hmapping[emapping[t]] for t in cl])\n",
    "    print 'detected', len(ecf), 'entity clusters'      \n",
    "    return ecf, voc\n",
    "\n",
    "def predict_select(model, MX, cluster):\n",
    "    CL = []\n",
    "    if MX.shape[0] == len(cluster):\n",
    "        for i in range(MX.shape[0]):\n",
    "            prediction = np.argmax(model.predict(MX[i]))\n",
    "            if prediction == 1:\n",
    "                CL.append(cluster[i])\n",
    "    else:\n",
    "        print 'unalligned matrices'\n",
    "    return CL\n",
    "\n",
    "def run_ngram_model(cdev, cprc):\n",
    "    print '____________________________________________________'\n",
    "    print 'running n-gram model'\n",
    "    wcorp = []\n",
    "    for i in cprc:\n",
    "        wcorp.append(' '.join(cprc[i]['words']))\n",
    "        \n",
    "    vectorizer = CountVectorizer(analyzer='word', binary=True, min_df=max(int(len(wcorp)*0.0005), 5), ngram_range=(2,3))\n",
    "    X = vectorizer.fit_transform(wcorp)\n",
    "    Xclean, mapping = filter_rare(X)\n",
    "    \n",
    "    Xdense = np.matrix(Xclean).astype('float')\n",
    "    X_scaled = preprocessing.scale(Xdense)\n",
    "    X_normalized = preprocessing.normalize(X_scaled, norm='l2')\n",
    "    \n",
    "    textMatrix = pairwise_distances(X_normalized, metric='cosine')\n",
    "    L = fastcluster.linkage(textMatrix, method='average')\n",
    "    flat_textclust = hierarchy.fcluster(L, 0.5, 'distance')\n",
    "    ttc = organize_clusters(flat_textclust)\n",
    "    \n",
    "    ncf = []\n",
    "    for cl in ttc:\n",
    "        ncf.append([mapping[t] for t in cl])\n",
    "    print 'detected', len(ncf), 'n-gram clusters'     \n",
    "    return ncf\n",
    "\n",
    "\n",
    "class CLST_PROC(object):\n",
    "    def __init__(self, model, pop_ent):\n",
    "        self.perceptron = model\n",
    "        self.voc = pop_ent\n",
    "    \n",
    "    def load_lang_models(self):\n",
    "        self.tfidf_vectorizer = pickle.load( open( \"tfidf_vectorizer\", \"rb\" ) )\n",
    "        self.w2v_dictionary = pickle.load( open( \"w2v_dictionary\", \"rb\" ) )\n",
    "        self.w2v_embeddings = pickle.load( open( \"w2v_embeddings\", \"rb\" ) )\n",
    "        \n",
    "    def process_cluster(self, dev, prc):\n",
    "        size = len(dev)\n",
    "        rawcorp = []\n",
    "        procorp = []\n",
    "        users = []\n",
    "        hashcorp = []\n",
    "        times = []\n",
    "        urls = []\n",
    "        mentions = []\n",
    "        retweets = []\n",
    "        followers = []\n",
    "        friends = []\n",
    "        status_cnt = []\n",
    "        crds = []\n",
    "        bagowords = []\n",
    "        prowcorp = []\n",
    "        checkins = []\n",
    "\n",
    "        for i in range(size):\n",
    "            pl = dev[i]['payload']\n",
    "            checkins += prc[i]['checkins']\n",
    "            bagowords += prc[i]['words']\n",
    "            prowcorp.append(prc[i]['words'])\n",
    "            procorp.append(' '.join(prc[i]['words']))\n",
    "            times.append(prc[i]['timestamp'])\n",
    "            [hashcorp.append(hs) for hs in prc[i]['hashtags']]\n",
    "            [urls.append(url) for url in prc[i]['urls']]\n",
    "            [mentions.append(ment) for ment in prc[i]['mentions']]\n",
    "\n",
    "            rawcorp.append(pl['text'])\n",
    "            users.append(pl['user']['id'])\n",
    "            retweets.append(pl['retweet_count'])\n",
    "            followers.append(pl['user']['followers_count'])\n",
    "            friends.append(pl['user']['friends_count'])\n",
    "            status_cnt.append(pl['user']['statuses_count'])\n",
    "            if pl['geo']:\n",
    "                crds.append(pl['geo']['coordinates'])\n",
    "\n",
    "        #pos_tokens = CMUTweetTagger.runtagger_parse(procorp)\n",
    "        #prop_nouns = float(str(pos_tokens).count('^'))/size\n",
    "        prop_nouns = 0\n",
    "\n",
    "        mtfidf_sim = np.mean(pdist(self.tfidf_vectorizer.transform(procorp).toarray(), lambda u, v: cosine_similarity(u,v)))\n",
    "        mw2v_sim = np.mean(pdist(w2v_transform(prowcorp), lambda u,v: cosine_similarity(u,v)))\n",
    "\n",
    "        pop_entities = float(len(list_intersection(hashcorp, voc)))\n",
    "        unq_unigrams = float(len(set(bagowords)))/size\n",
    "        htags = float(len(hashcorp))/size\n",
    "        unq_htags = len(set(hashcorp))\n",
    "        unq_checkins = len(set(checkins))\n",
    "        checkins = float(len(checkins))/size\n",
    "\n",
    "        cs = np.array(crds)\n",
    "        try:\n",
    "            bboxsquare = (max(cs[:,0])-min(cs[:,0]))*(max(cs[:,1])-min(cs[:,1]))\n",
    "        except:\n",
    "            bboxsquare = 0\n",
    "\n",
    "        unq_users = float(len(set(users)))/size\n",
    "        meanfollowers = np.mean(followers)\n",
    "        meanfriends = np.mean(friends)\n",
    "        retweets = sum(retweets)\n",
    "        mentions = float(len(mentions))/size\n",
    "\n",
    "        timeframe = float(max(times)-min(times))/60\n",
    "        inst_urls = float(str(urls).count('instagram'))/size\n",
    "        urls = float(len(urls))/size\n",
    "\n",
    "        features = {'size':size, 'prop_nouns':prop_nouns, 'pop_entities':pop_entities, 'unq_unigrams':unq_unigrams,\n",
    "                    'hashtags':htags, 'unq_users':unq_users, 'unq_hashtags':unq_htags, 'bbox':bboxsquare, \n",
    "                    'timeframe':timeframe, 'mentions':mentions, 'mfollowers':meanfollowers, 'mfriends':meanfriends, \n",
    "                    'retweets':retweets, 'urls':urls, 'inst_urls':inst_urls,'mtfidf_sim':mtfidf_sim, \n",
    "                    'mw2v_sim':mw2v_sim, 'unq_checkins': unq_checkins, 'checkins': checkins}\n",
    "\n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def logistic_derivative(x):\n",
    "    return logistic(x)*(1-logistic(x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def load_model(self):\n",
    "        self.layers = pickle.load( open( \"assets/layers\", \"rb\" ) )\n",
    "        self.weights = pickle.load( open( \"assets/weights\", \"rb\" ) )\n",
    "    \n",
    "    def __init__(self, layers, weights = [], activation='logistic'):\n",
    "\n",
    "        if activation == 'logistic':\n",
    "            self.activation = logistic\n",
    "            self.activation_deriv = logistic_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_deriv = tanh_deriv\n",
    "\n",
    "        self.weights = weights\n",
    "        if not weights:\n",
    "            for i in range(1, len(layers) - 1):\n",
    "                self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)\n",
    "            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n",
    "        \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=10000):\n",
    "        X = np.atleast_2d(X)\n",
    "        temp = np.ones([X.shape[0], X.shape[1]+1])\n",
    "        temp[:, 0:-1] = X  # adding the bias unit to the input layer\n",
    "        X = temp\n",
    "        y = np.array(y)\n",
    "\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                a.append(self.activation(np.dot(a[l], self.weights[l])))\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_deriv(a[-1])]\n",
    "\n",
    "            for layer in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer\n",
    "                deltas.append(deltas[-1].dot(self.weights[layer].T)*self.activation_deriv(a[layer]))\n",
    "            deltas.reverse()\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "                \n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        temp = np.ones(x.shape[0]+1)\n",
    "        temp[0:-1] = x\n",
    "        a = temp\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected\n",
      "querying..\n",
      "retrieving..\n",
      "16800\n",
      "retrieved 16807 documents from window\n",
      "filtered 8080 documents, 8727 left\n",
      "____________________________________________________\n",
      "running geo model\n",
      "filtered 7184 documents, 1543 left\n",
      "detected 103 geo clusters\n",
      "____________________________________________________\n",
      "running entity model\n",
      "removed 7424 documents 1303 left\n",
      "100.00%\n",
      "100.00%\n",
      "detected 24 entity clusters\n",
      "____________________________________________________\n",
      "running n-gram model\n",
      "100.00%\n",
      "100.00%\n",
      "detected 19 n-gram clusters\n",
      "100.00%\n",
      "100.00%\n",
      "100.00%\n",
      "100.00%\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 20:56:33.928571\n",
      "location: [-77.60725867  43.14884102]\n",
      "popular hashtags: [(u'rochester', 13), (u'photo', 7), (u'StepJam2015', 4), (u'WDKX', 4), (u'Rochester', 4)]\n",
      "popular checkins: [(u'blue', 4), (u'arena', 4), (u'cross', 4)]\n",
      "https://www.instagram.com/p/_NAwT4nEUd/\n",
      "https://www.instagram.com/p/_NA_ZAuWNC/\n",
      "https://www.instagram.com/p/_NCyyGuWA0/\n",
      "https://www.instagram.com/p/_NDImaMZ4G/\n",
      "https://www.instagram.com/p/_NDsg_uWC5/\n",
      "https://www.instagram.com/p/_NEaskuWEQ/\n",
      "https://www.instagram.com/p/_NE0PXuWFZ/\n",
      "https://www.instagram.com/p/_NF8v_GyIA/\n",
      "https://www.instagram.com/p/_NGG8qmyIX/\n",
      "https://www.instagram.com/p/_NGVcjGyI9/\n",
      "https://www.instagram.com/p/_NQOOBS7Yj/\n",
      "https://www.instagram.com/p/_NRFNnILOt/\n",
      "https://www.instagram.com/p/_NSUAJElEF/\n",
      "https://www.instagram.com/p/_NSrjBimZh/\n",
      "https://www.instagram.com/p/_NTdSdS7ff/\n",
      "https://www.instagram.com/p/_NXy2FwFq19Am57_OpNfnCoS4Izh7lOn1ZKmw0/\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 20:12:30.428571\n",
      "location: [-70.23532657  44.13351343]\n",
      "popular hashtags: [(u'mesports', 14), (u'cm', 8), (u'SJbasketball', 7), (u'sjbasketball', 7), (u'st', 5)]\n",
      "popular checkins: []\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 21:31:38.857143\n",
      "location: [-71.3033368   41.74554335]\n",
      "popular hashtags: [(u'pottery', 9), (u'paint', 4), (u'party', 4), (u'birthday', 4), (u'painting', 4)]\n",
      "popular checkins: [(u'pottery', 1), (u'art', 1), (u'club', 1)]\n",
      "https://www.instagram.com/p/_NCRIMNodX/\n",
      "https://www.instagram.com/p/_NOfY0tr09/\n",
      "https://www.instagram.com/p/_NPCwhNr1_/\n",
      "https://www.instagram.com/p/_NP5Z-tr33/\n",
      "https://www.instagram.com/p/_NQERQNr4S/\n",
      "https://www.instagram.com/p/_NWk09hHfR/\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 19:56:16\n",
      "location: [-72.253334   41.8049316]\n",
      "popular hashtags: [(u'gampel', 4), (u'pavilion', 3), (u'uconn', 2), (u'go', 2), (u'godawgs', 2)]\n",
      "popular checkins: [(u'pavilion', 3), (u'gampel', 3)]\n",
      "https://www.instagram.com/p/_NDy2wB3Fe/\n",
      "https://www.instagram.com/p/_NE6t7GpZl2oYAWUB3dDDIchR980qbI5XRZOk0/\n",
      "https://www.instagram.com/p/_NE3erClyHkeJOM-UPAsXeivFogPlJIWz3jfE0/\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 20:37:38.400000\n",
      "location: [-69.8601401  44.4499815]\n",
      "popular hashtags: [(u'mesports', 20), (u'cm', 8), (u'sjbasketball', 7), (u'SJbasketball', 7), (u'leads', 5)]\n",
      "popular checkins: []\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 19:59:28.142857\n",
      "location: [-71.0887545  42.3136695]\n",
      "popular hashtags: [(u'jobsjusticeclimate', 14), (u'FightFor15', 2), (u'justice', 2), (u'create', 2), (u'fightfor15', 2)]\n",
      "popular checkins: []\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 21:11:10.166667\n",
      "location: [-71.0618882   42.36588088]\n",
      "popular hashtags: [(u'td', 6), (u'garden', 6), (u'everything', 2), (u'newestbruinsfan', 2), (u'concert', 1)]\n",
      "popular checkins: [(u'td', 6), (u'garden', 6)]\n",
      "https://www.instagram.com/p/_NG6q1TaPe/\n",
      "https://www.instagram.com/p/_NKWHHnaQi/\n",
      "https://www.instagram.com/p/_NMLphhROIlKR6ez-6qlbWoVX9dilAq5ZJCgk0/\n",
      "https://www.instagram.com/p/_NPyLdHBAU/\n",
      "https://www.instagram.com/p/_NQ-8tLzDY/\n",
      "https://www.instagram.com/p/_NRixoCVdl/\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 20:29:52.500000\n",
      "location: [-71.0618882   42.36588088]\n",
      "popular hashtags: [(u'nhlbruins', 6), (u'garden', 5), (u'td', 4), (u'NHLBruins', 2), (u'munchkins2015', 2)]\n",
      "popular checkins: [(u'td', 4), (u'garden', 4)]\n",
      "https://www.instagram.com/p/_NBfKrIWvZAX8libeElAV1B0Z-djOrbgYHy900/\n",
      "https://www.instagram.com/p/_NGN8TlqMK/\n",
      "https://www.instagram.com/p/_NKmFRJrrZ/\n",
      "https://www.instagram.com/p/_NP-M4SRSFLbOnV_n6MyFLsVo8oAdWEmB3lHk0/\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 20:19:15.666667\n",
      "location: [-71.0887545  42.3136695]\n",
      "popular hashtags: [(u'panthers', 17), (u'bruins', 17), (u'nhl', 16), (u'MetroBoston', 15), (u'Bruins', 15)]\n",
      "popular checkins: []\n",
      "-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
      "time: 2015-12-12 20:36:24.200000\n",
      "location: [-72.51810993  43.62424866]\n",
      "popular hashtags: [(u'woodstock', 5), (u'parade', 4), (u'ho', 3), (u'town', 2), (u'vermont', 2)]\n",
      "popular checkins: [(u'woodstock', 4), (u'town', 2), (u'vermont', 1)]\n",
      "https://www.instagram.com/p/_NG187PcBtHv4AMqX8h-v8zECeh575SFZqXS80/\n",
      "https://www.instagram.com/p/_NG_fPvcCPumg-I-dF0CdEvn-uMlmCBPp8iu80/\n",
      "https://www.instagram.com/p/_NHHwgPcCxnqFdfpBTyr3t8wjJrfIxWCaQV2c0/\n",
      "https://www.instagram.com/p/_NHqXIJKsb/\n",
      "https://www.instagram.com/p/_NR90wQjo4a9LnSSv-gkPRv75WcOi-cRmaPVk0/\n"
     ]
    }
   ],
   "source": [
    "from tweet_proc import *\n",
    "\n",
    "msc = WD_PROC_7000('tweets', 1, port = 27000)\n",
    "\n",
    "startEpoch = 'Sat Dec 12 14:00:00 +0000 2015'\n",
    "endEpoch = 'Sat Dec 12 23:00:00 +0000 2015'\n",
    "\n",
    "dev = retrieve_from_timewindow(msc, 'Boston', startEpoch, endEpoch)\n",
    "cdev, cprc = filter_noisy(msc, dev, w = 3, h = 6)\n",
    "\n",
    "geoclusters = run_geoclust_model(cdev, cprc)\n",
    "entityclusters, voc = run_entity_model(cdev, cprc)\n",
    "ngramclusters = run_ngram_model(cdev, cprc)\n",
    "\n",
    "model = NeuralNetwork([], 'logistic')\n",
    "model.load_model()\n",
    "\n",
    "ccp = CLST_PROC(model, voc)\n",
    "ccp.load_lang_models()\n",
    "\n",
    "GX = vectorize_clusters(cdev, cprc, geoclusters, ccp)\n",
    "EX = vectorize_clusters(cdev, cprc, entityclusters, ccp)\n",
    "NX = vectorize_clusters(cdev, cprc, ngramclusters, ccp)\n",
    "\n",
    "fGX = predict_select(model, GX, geoclusters)\n",
    "fEX = predict_select(model, EX, entityclusters)\n",
    "fNX = predict_select(model, NX, ngramclusters)\n",
    "\n",
    "for i in fGX:\n",
    "    t = materialize(cdev, cprc, i)\n",
    "    summarize(t)\n",
    "    \n",
    "for i in fEX:\n",
    "    t = materialize(cdev, cprc, i)\n",
    "    summarize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize(event):\n",
    "    print '-----#-----#-----#-----#-----#-----#-----#-----#-----'\n",
    "    bagowords = []\n",
    "    bagohashs = []\n",
    "    bagocheckins = []\n",
    "    instlinks = []\n",
    "    crds = []\n",
    "    times = []\n",
    "    for p in event['prc']:\n",
    "        bagowords += p['words']\n",
    "        times.append(p['timestamp'])\n",
    "        bagohashs += p['hashtags']\n",
    "        bagocheckins += p['checkins']\n",
    "        [instlinks.append(l) for l in p['urls'] if 'instagram' in l]\n",
    "        if p['crds']:\n",
    "            crds.append(p['crds'])\n",
    "            \n",
    "    print 'time:', datetime.datetime.fromtimestamp(np.mean(times))\n",
    "    if len(crds):\n",
    "        print 'location:', sum(np.array(crds))/len(crds)\n",
    "    print 'popular hashtags:', collections.Counter(bagowords+bagohashs).most_common(5)\n",
    "    print 'popular checkins:', collections.Counter(bagocheckins).most_common(3)\n",
    "    for l in instlinks:\n",
    "        print l\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hand-labelling clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweet_proc import *\n",
    "\n",
    "msc = WD_PROC_7000('tweets', 1, port = 27000)\n",
    "\n",
    "startEpoch = 'Sat Dec 12 14:00:00 +0000 2015'\n",
    "endEpoch = 'Sat Dec 12 23:00:00 +0000 2015'\n",
    "\n",
    "dev = retrieve_from_timewindow(msc, 'Boston', startEpoch, endEpoch)\n",
    "cdev, cprc = filter_noisy(msc, dev, w = 3, h = 6)\n",
    "\n",
    "geoclusters = run_geoclust_model(cdev, cprc)\n",
    "entityclusters, voc = run_entity_model(cdev, cprc)\n",
    "ngramclusters = run_ngram_model(cdev, cprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_cluster(cluster, mode = 'mixed'):\n",
    "\n",
    "    print mode, 'mode, already found total', len(events), 'events and', len(spam), 'spam'\n",
    "    print '__________________________________________________________________________________________________'\n",
    "    tdev = []\n",
    "    tprc = []\n",
    "    for p in cluster:\n",
    "        print cdev[p]['payload']['text']\n",
    "        tdev.append(cdev[p])\n",
    "        tprc.append(cprc[p])\n",
    "    \n",
    "    decision = raw_input()\n",
    "    \n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    if decision == '1':\n",
    "        pc = ccp.process_cluster(tdev, tprc)\n",
    "        events.append({'event':True, 'processed':pc, 'dev':tdev, 'prc':tprc, 'model': mode, 'date':startEpoch})\n",
    "        #sevents.append({'event':True, 'processed':pc, 'model':mode, 'date':startEpoch})\n",
    "    if decision == '2':\n",
    "        pc = ccp.process_cluster(tdev, tprc)\n",
    "        spam.append({'event':False, 'processed':pc, 'dev':tdev, 'prc':tprc, 'model': mode, 'date':startEpoch})\n",
    "        #sspam.append({'event':False, 'processed':pc, 'model':mode, 'date':startEpoch})\n",
    "    if decision == '3':\n",
    "        pass\n",
    "    if decision == '4':\n",
    "        return 7\n",
    "    \n",
    "def relabel_cluster(cluster):\n",
    "\n",
    "    print '__________________________________________________________________________________________________'\n",
    "    tdev = cluster['dev']\n",
    "    tprc = cluster['prc']\n",
    "    for p in tdev:\n",
    "        print p['payload']['text']\n",
    "    \n",
    "    decision = raw_input()\n",
    "    \n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    if decision == '1':\n",
    "        return 1\n",
    "    if decision == '2':\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in geoclusters[60:]:\n",
    "    t = label_cluster(cluster, 'geo')\n",
    "    if t == 7:\n",
    "        break\n",
    "        \n",
    "for cluster in entityclusters[:50]:\n",
    "    t = label_cluster(cluster, 'entity')\n",
    "    if t == 7:\n",
    "        break\n",
    "        \n",
    "for cluster in ngramclusters[:35]:\n",
    "    t = label_cluster(cluster, 'ngram')\n",
    "    if t == 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, event in enumerate(cleanevents):\n",
    "    try:\n",
    "        devl = event['dev']\n",
    "        prcl = event['prc']\n",
    "\n",
    "        tdev = []\n",
    "        tprc = []\n",
    "        for t in range(len(devl)):\n",
    "            tdev.append(devl[t])\n",
    "            tprc.append(prcl[t])\n",
    "\n",
    "        pc = ccp.process_cluster(tdev,tprc)\n",
    "\n",
    "        event['processed'] = pc\n",
    "    except:\n",
    "        print i\n",
    "    #print event['processed']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleanevents = pickle.load( open( \"assets/cleanevents\", \"rb\" ) )\n",
    "spam = pickle.load( open( \"assets/spam\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EX = []\n",
    "for c in cleanevents:\n",
    "    v = c['processed'].values()\n",
    "    v.append(1)\n",
    "    EX.append(v)\n",
    "EX = np.array(EX)\n",
    "EX = np.concatenate(2*[EX])\n",
    "EX = np.array(random.sample(EX, 300))\n",
    "\n",
    "SX = []\n",
    "for c in spam:\n",
    "    v = c['processed'].values()\n",
    "    v.append(2)\n",
    "    SX.append(v)\n",
    "SX = np.array(SX)\n",
    "\n",
    "SMX = np.array(random.sample(SX, 300))\n",
    "\n",
    "dataset = np.concatenate([EX, SMX])\n",
    "\n",
    "data = dataset[:,0:-1]\n",
    "targets = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "rep = {}\n",
    "rz = 0\n",
    "#X = data\n",
    "#X = preprocessing.scale(X)\n",
    "lrs = [[19,20,20,2],[19,22,22,2],[19,25,25,2],[19,27,27,2],\n",
    "       [19,30,30,2],[19,32,32,2],[19,34,34,2]]\n",
    "\n",
    "rzs = {}\n",
    "for i in lrs:\n",
    "    rzs[i[1]] = []\n",
    "\n",
    "ret = 5\n",
    "\n",
    "X = preprocessing.scale(data)\n",
    "X = preprocessing.normalize(X, norm='l2')\n",
    "y = targets\n",
    "\n",
    "for i, lr in enumerate(lrs):\n",
    "    print i, '/', len(lrs)\n",
    "    for z in range(ret):\n",
    "        progress(z, ret, skip = 1)\n",
    "        \n",
    "        layers = lr\n",
    "        nn = NeuralNetwork(layers, activation = 'logistic')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "        labels_test = LabelBinarizer().fit_transform(y_test)\n",
    "\n",
    "        nn.fit(X_train,labels_train,epochs=3000)\n",
    "        predictions = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            o = nn.predict(X_test[i] )\n",
    "            predictions.append(np.argmax(o)+1)\n",
    "\n",
    "        report = classification_report(y_test,predictions)\n",
    "        t = [float(t) for t in report.split('total')[1][7:31].split('   ') if t!='']\n",
    "        rzs[layers[1]].append(t)\n",
    "\n",
    "\n",
    "\n",
    "        if sum(t[1:])>rz:\n",
    "            rz = sum(t[1:])\n",
    "            bestweights = nn.weights\n",
    "            bestreport = report\n",
    "            bestlr = layers\n",
    "        #rep[str(layers)].append(t)\n",
    "\n",
    "print bestreport\n",
    "#print classification_report(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Boosting bursty n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocX = vectorizer.get_feature_names()\n",
    "boost_entity = boost_entities(vocX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqTwCl = Counter(flat_textclust)\n",
    "print \"n_clusters:\", len(freqTwCl)\n",
    "\n",
    "npindL = np.array(indL)\n",
    "\n",
    "freq_th = max(5, int(X.shape[0]*0.0005))\n",
    "cluster_score = {}\n",
    "for clfreq in freqTwCl.most_common(50):\n",
    "    cl = clfreq[0]\n",
    "    freq = clfreq[1]\n",
    "    cluster_score[cl] = 0\n",
    "    if freq >= freq_th:\n",
    "        clidx = (npindL == cl).nonzero()[0].tolist()\n",
    "        cluster_centroid = X[clidx].sum(axis=0)\n",
    "        try:\n",
    "            cluster_tweet = vectorizer.inverse_transform(cluster_centroid)\n",
    "            for term in np.nditer(cluster_tweet):\n",
    "                try:\n",
    "                    cluster_score[cl] = max(cluster_score[cl], boosted_wdfVoc[str(term).strip()])\n",
    "                except:\n",
    "                    pass        \n",
    "        except: pass\n",
    "        cluster_score[cl] /= freq\n",
    "    else: break\n",
    "\n",
    "sorted_clusters = sorted( ((v,k) for k,v in cluster_score.iteritems()), reverse=True)\n",
    "print \"sorted cluster_score:\"\n",
    "print sorted_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfX = Xclean.sum(axis=0)\n",
    "dfVoc = {}\n",
    "wdfVoc = {}\n",
    "dfVocTimeWindows = {}\n",
    "boosted_wdfVoc = {}\n",
    "keys = vocX\n",
    "vals = dfX\n",
    "t = 4\n",
    "for k,v in zip(keys, vals):\n",
    "    dfVoc[k] = v\n",
    "    \n",
    "for k in dfVoc: \n",
    "    try:\n",
    "        \n",
    "        dfVocTimeWindows[k] += dfVoc[k]\n",
    "        avgdfVoc = (dfVocTimeWindows[k] - dfVoc[k])/(t - 1)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        dfVocTimeWindows[k] = dfVoc[k]\n",
    "        avgdfVoc = 0\n",
    " \n",
    "    wdfVoc[k] = (dfVoc[k] + 1) / (np.log(avgdfVoc + 1) + 1)\n",
    "    try:\n",
    "        boosted_wdfVoc[k] = wdfVoc[k] * boost_entity[k]\n",
    "    except: \n",
    "        boosted_wdfVoc[k] = wdfVoc[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
